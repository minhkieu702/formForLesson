import os
import gspread
from oauth2client.service_account import ServiceAccountCredentials
import pandas as pd
import lightgbm as lgb
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
import joblib

print("Báº¯t Ä‘áº§u quÃ¡ trÃ¬nh huáº¥n luyá»‡n model...")

# --- Káº¾T Ná»I Vá»šI GOOGLE SHEETS ---
try:
    scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']

    # Tá»° Äá»˜NG TÃŒM ÄÆ¯á»œNG DáºªN Äáº¾N FILE CREDENTIALS
    # Láº¥y Ä‘Æ°á»ng dáº«n tuyá»‡t Ä‘á»‘i Ä‘áº¿n thÆ° má»¥c chá»©a script Ä‘ang cháº¡y (AImodel)
    script_dir = os.path.dirname(os.path.abspath(__file__))
    # Táº¡o Ä‘Æ°á»ng dáº«n Ä‘áº§y Ä‘á»§ vÃ  chÃ­nh xÃ¡c Ä‘áº¿n file credentials.json
    credentials_path = os.path.join(script_dir, 'credentials.json')

    print(f"Äang tÃ¬m file credentials táº¡i Ä‘Æ°á»ng dáº«n: {credentials_path}")

    creds = ServiceAccountCredentials.from_json_keyfile_name(credentials_path, scope)
    client = gspread.authorize(creds)

    # THAY TÃŠN FILE GOOGLE SHEETS Cá»¦A Báº N VÃ€O ÄÃ‚Y
    SHEET_NAME = "INS influencer_posts"
    spreadsheet = client.open(SHEET_NAME)
    print(f"ÄÃ£ káº¿t ná»‘i thÃ nh cÃ´ng tá»›i Google Sheet: '{SHEET_NAME}'")

except FileNotFoundError:
    print(f"Lá»–I Káº¾T Ná»I: KhÃ´ng tÃ¬m tháº¥y file 'credentials.json'.")
    print("Vui lÃ²ng Ä‘áº£m báº£o file 'credentials.json' náº±m cÃ¹ng thÆ° má»¥c vá»›i file 'train_model.py'.")
    exit()
except Exception as e:
    print(f"Lá»–I Káº¾T Ná»I: {e}")
    exit()

# --- Táº¢I Dá»® LIá»†U Tá»ª SHEET 'influencer_posts' ---
try:
    sheet = spreadsheet.worksheet('influencer_posts')
    records = sheet.get_all_records()
    df_posts = pd.DataFrame(records)
    print(f"ÄÃ£ táº£i thÃ nh cÃ´ng {len(df_posts)} dÃ²ng tá»« sheet 'influencer_posts'.")
except gspread.exceptions.WorksheetNotFound:
    print("Lá»–I: KhÃ´ng tÃ¬m tháº¥y sheet 'influencer_posts'. Vui lÃ²ng kiá»ƒm tra láº¡i tÃªn sheet.")
    exit()

# --- Xá»¬ LÃ Dá»® LIá»†U ---
# Chuyá»ƒn Ä‘á»•i cÃ¡c cá»™t sá»‘
numeric_cols = ['Likes', 'Comments', 'Shares', 'Followers', 'Post Reach']
for col in numeric_cols:
    df_posts[col] = pd.to_numeric(df_posts[col], errors='coerce')

# Chuyá»ƒn Ä‘á»•i cá»™t ngÃ y thÃ¡ng
df_posts['Post Date'] = pd.to_datetime(df_posts['Post Date'], errors='coerce')
df_posts.dropna(subset=numeric_cols + ['Post Date'], inplace=True)
df_posts['Hashtags'] = df_posts['Hashtags'].astype(str) # Äáº£m báº£o Hashtags lÃ  chuá»—i

# --- HUáº¤N LUYá»†N MODEL 1: Dá»° ÄOÃN HIá»†U SUáº¤T BÃ€I ÄÄ‚NG ---
print("\n--- Huáº¥n luyá»‡n Model 1: Dá»± Ä‘oÃ¡n Post Reach ---")

# Feature Engineering
df_posts['day_of_week'] = df_posts['Post Date'].dt.dayofweek
df_posts['hour_of_day'] = df_posts['Post Date'].dt.hour

features = ['Influencer ID', 'Content Type', 'Followers', 'day_of_week', 'hour_of_day', 'Hashtags']
target = 'Post Reach'

X = df_posts[features]
y = df_posts[target]

# Pipeline xá»­ lÃ½
preprocessor = ColumnTransformer(
    transformers=[
        ('cat', OneHotEncoder(handle_unknown='ignore'), ['Influencer ID', 'Content Type']),
        ('text', TfidfVectorizer(min_df=1), 'Hashtags')
    ],
    remainder='passthrough'
)

model_pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('regressor', lgb.LGBMRegressor(random_state=42, verbosity=-1))
])

# Huáº¥n luyá»‡n model trÃªn toÃ n bá»™ dá»¯ liá»‡u hiá»‡n cÃ³
model_pipeline.fit(X, y)
joblib.dump(model_pipeline, 'post_performance_predictor.joblib')
print("âœ… Model 1 Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n vÃ  lÆ°u vÃ o 'post_performance_predictor.joblib'")
print("\nğŸ‰ HoÃ n táº¥t quÃ¡ trÃ¬nh huáº¥n luyá»‡n!")